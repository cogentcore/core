// Code generated by "core generate"; DO NOT EDIT.

package lexer

import (
	"cogentcore.org/core/text/token"
	"cogentcore.org/core/tree"
	"cogentcore.org/core/types"
)

var _ = types.AddType(&types.Type{Name: "cogentcore.org/core/text/parse/lexer.Rule", IDName: "rule", Doc: "Rule operates on the text input to produce the lexical tokens.\n\nLexing is done line-by-line -- you must push and pop states to\ncoordinate across multiple lines, e.g., for multi-line comments.\n\nThere is full access to entire line and you can decide based on future\n(offset) characters.\n\nIn general it is best to keep lexing as simple as possible and\nleave the more complex things for the parsing step.", Embeds: []types.Field{{Name: "NodeBase"}}, Fields: []types.Field{{Name: "Off", Doc: "disable this rule -- useful for testing and exploration"}, {Name: "Desc", Doc: "description / comments about this rule"}, {Name: "Token", Doc: "the token value that this rule generates -- use None for non-terminals"}, {Name: "Match", Doc: "the lexical match that we look for to engage this rule"}, {Name: "Pos", Doc: "position where match can occur"}, {Name: "String", Doc: "if action is LexMatch, this is the string we match"}, {Name: "Offset", Doc: "offset into the input to look for a match: 0 = current char, 1 = next one, etc"}, {Name: "SizeAdj", Doc: "adjusts the size of the region (plus or minus) that is processed for the Next action -- allows broader and narrower matching relative to tagging"}, {Name: "Acts", Doc: "the action(s) to perform, in order, if there is a match -- these are performed prior to iterating over child nodes"}, {Name: "Until", Doc: "string(s) for ReadUntil action -- will read until any of these strings are found -- separate different options with | -- if you need to read until a literal | just put two || in a row and that will show up as a blank, which is interpreted as a literal |"}, {Name: "PushState", Doc: "the state to push if our action is PushState -- note that State matching is on String, not this value"}, {Name: "NameMap", Doc: "create an optimization map for this rule, which must be a parent with children that all match against a Name string -- this reads the Name and directly activates the associated rule with that String, without having to iterate through them -- use this for keywords etc -- produces a SIGNIFICANT speedup for long lists of keywords."}, {Name: "MatchLen", Doc: "length of source that matched -- if Next is called, this is what will be skipped to"}, {Name: "NmMap", Doc: "NameMap lookup map -- created during Compile"}}})

// NewRule returns a new [Rule] with the given optional parent:
// Rule operates on the text input to produce the lexical tokens.
//
// Lexing is done line-by-line -- you must push and pop states to
// coordinate across multiple lines, e.g., for multi-line comments.
//
// There is full access to entire line and you can decide based on future
// (offset) characters.
//
// In general it is best to keep lexing as simple as possible and
// leave the more complex things for the parsing step.
func NewRule(parent ...tree.Node) *Rule { return tree.New[Rule](parent...) }

// SetOff sets the [Rule.Off]:
// disable this rule -- useful for testing and exploration
func (t *Rule) SetOff(v bool) *Rule { t.Off = v; return t }

// SetDesc sets the [Rule.Desc]:
// description / comments about this rule
func (t *Rule) SetDesc(v string) *Rule { t.Desc = v; return t }

// SetToken sets the [Rule.Token]:
// the token value that this rule generates -- use None for non-terminals
func (t *Rule) SetToken(v token.Tokens) *Rule { t.Token = v; return t }

// SetMatch sets the [Rule.Match]:
// the lexical match that we look for to engage this rule
func (t *Rule) SetMatch(v Matches) *Rule { t.Match = v; return t }

// SetPos sets the [Rule.Pos]:
// position where match can occur
func (t *Rule) SetPos(v MatchPos) *Rule { t.Pos = v; return t }

// SetString sets the [Rule.String]:
// if action is LexMatch, this is the string we match
func (t *Rule) SetString(v string) *Rule { t.String = v; return t }

// SetOffset sets the [Rule.Offset]:
// offset into the input to look for a match: 0 = current char, 1 = next one, etc
func (t *Rule) SetOffset(v int) *Rule { t.Offset = v; return t }

// SetSizeAdj sets the [Rule.SizeAdj]:
// adjusts the size of the region (plus or minus) that is processed for the Next action -- allows broader and narrower matching relative to tagging
func (t *Rule) SetSizeAdj(v int) *Rule { t.SizeAdj = v; return t }

// SetActs sets the [Rule.Acts]:
// the action(s) to perform, in order, if there is a match -- these are performed prior to iterating over child nodes
func (t *Rule) SetActs(v ...Actions) *Rule { t.Acts = v; return t }

// SetUntil sets the [Rule.Until]:
// string(s) for ReadUntil action -- will read until any of these strings are found -- separate different options with | -- if you need to read until a literal | just put two || in a row and that will show up as a blank, which is interpreted as a literal |
func (t *Rule) SetUntil(v string) *Rule { t.Until = v; return t }

// SetPushState sets the [Rule.PushState]:
// the state to push if our action is PushState -- note that State matching is on String, not this value
func (t *Rule) SetPushState(v string) *Rule { t.PushState = v; return t }

// SetNameMap sets the [Rule.NameMap]:
// create an optimization map for this rule, which must be a parent with children that all match against a Name string -- this reads the Name and directly activates the associated rule with that String, without having to iterate through them -- use this for keywords etc -- produces a SIGNIFICANT speedup for long lists of keywords.
func (t *Rule) SetNameMap(v bool) *Rule { t.NameMap = v; return t }

// SetMatchLen sets the [Rule.MatchLen]:
// length of source that matched -- if Next is called, this is what will be skipped to
func (t *Rule) SetMatchLen(v int) *Rule { t.MatchLen = v; return t }

// SetNmMap sets the [Rule.NmMap]:
// NameMap lookup map -- created during Compile
func (t *Rule) SetNmMap(v map[string]*Rule) *Rule { t.NmMap = v; return t }
